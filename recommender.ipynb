{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLNqYBgmGIyf"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hzzVAc5GBA_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "import threading\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize.regexp import (WordPunctTokenizer,wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vH15YyQGM1I"
   },
   "source": [
    "Links for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kj91r1mXGRPD"
   },
   "outputs": [],
   "source": [
    "url_links = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/links.csv'\n",
    "url_movies = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/movies.csv'\n",
    "url_ratings = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/ratings.csv'\n",
    "url_tags = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/tags.csv'\n",
    "url_genres = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/user_genre.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7uqFvueudeU"
   },
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUqBO0kkucLC"
   },
   "outputs": [],
   "source": [
    "ds_links = pd.read_csv(url_links, dtype = str)\n",
    "ds_movies = pd.read_csv(url_movies, dtype = str)\n",
    "ds_ratings = pd.read_csv(url_ratings)\n",
    "ds_tags = pd.read_csv(url_tags, dtype = str)\n",
    "ds_user_genres = pd.read_json(url_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxrT24702OZK"
   },
   "outputs": [],
   "source": [
    "list_movies_id = list(ds_movies['movieId'].unique())\n",
    "list_title_id = ds_movies['title'].tolist()\n",
    "list_users_id = list(ds_ratings['userId'].unique())\n",
    "list_movies_imdbid = ds_links['imdbId'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCHdeaAxoy5i"
   },
   "source": [
    "Input ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "O_MY4EUno1II",
    "outputId": "2507215a-d421-4bff-dae7-7bdce5d22934"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "movie_ids=[]\n",
    "ratings=[]\n",
    "\n",
    "for i in range(10):\n",
    "  index=random.randint(0,len(list_movies_id)-1)\n",
    "  while list_movies_id[index] in movie_ids:\n",
    "    index=random.randint(0,len(list_movies_id)-1)\n",
    "  rating = float(input('Rate the movie '+str(list_title_id[index])+' from 0.5 to 5.0 or say 0.0 if you haven\\'t seen it :'))\n",
    "  movie_ids.append(list_movies_id[index])\n",
    "  ratings.append(rating)\n",
    "print(movie_ids)\n",
    "print(ratings)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfYUIC48Cta9"
   },
   "source": [
    "Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zY9Fa_VcLVa"
   },
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szoO5K7IKg5e"
   },
   "outputs": [],
   "source": [
    "def getGenresByUser(user_id, ds=ds_user_genres):\n",
    "  return ds[user_id]['like'], ds[user_id]['dislike']\n",
    "\n",
    "print(getGenresByUser(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRrF5bPD_bZs"
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "\n",
    "test_ratio = 0.2 #fraction of data to be used as test set.\n",
    "no_of_features = [8,10,12,14,17,20] # to test the performance over a different number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVL4pD2A9zIu"
   },
   "outputs": [],
   "source": [
    "ds_ratings['userId'] = ds_ratings['userId'].astype('str')\n",
    "ds_ratings['movieId'] = ds_ratings['movieId'].astype('str')\n",
    "\n",
    "users = ds_ratings['userId'].unique() #list of all users\n",
    "movies = ds_ratings['movieId'].unique() #list of all movies\n",
    "\n",
    "test = pd.DataFrame(columns=ds_ratings.columns)\n",
    "train = pd.DataFrame(columns=ds_ratings.columns)\n",
    "\n",
    "for u in users:\n",
    "  temp = ds_ratings[ds_ratings['userId'] == u]\n",
    "  n = len(temp)\n",
    "  test_size = int(test_ratio*n)\n",
    "\n",
    "  temp = temp.sort_values('timestamp').reset_index()\n",
    "  temp.drop('index', axis=1, inplace=True)\n",
    "\n",
    "  dummy_test = temp.loc[n-1-test_size :]\n",
    "  dummy_train = temp.loc[: n-2-test_size]\n",
    "    \n",
    "  test = pd.concat([test, dummy_test])\n",
    "  train = pd.concat([train, dummy_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvRLP6__-98r"
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def create_user_item_matrix(data, formatizer = {'user':0, 'item': 1, 'value': 2}):\n",
    "    itemField = formatizer['item']\n",
    "    userField = formatizer['user']\n",
    "    valueField = formatizer['value']\n",
    "\n",
    "    \n",
    "    userList = data.iloc[:,userField].tolist()\n",
    "    itemList = data.iloc[:,itemField].tolist()\n",
    "    valueList = data.iloc[:,valueField].tolist()\n",
    "\n",
    "    users = list(set(data.iloc[:,userField]))\n",
    "    items = list(set(data.iloc[:,itemField]))\n",
    "\n",
    "    users_index = {users[i]: i for i in range(len(users))}\n",
    "\n",
    "    pd_dict = {item: [np.nan for i in range(len(users))] for item in items}\n",
    "\n",
    "    for i in range(0,len(data)):\n",
    "      item = itemList[i]\n",
    "      user = userList[i]\n",
    "      value = valueList[i]\n",
    "\n",
    "      pd_dict[item][users_index[user]] = value\n",
    "    \n",
    "    X = pd.DataFrame(pd_dict)\n",
    "    X.index = users\n",
    "\n",
    "    itemcols = list(X.columns)\n",
    "    items_index = {itemcols[i]: i for i in range(len(itemcols))}\n",
    "\n",
    "    # users_index gives us a mapping of user_id to index of user\n",
    "    # items_index provides the same for items\n",
    "\n",
    "    return X, users_index, items_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tr-QPs9A_K6a"
   },
   "outputs": [],
   "source": [
    "def svd(train, k):\n",
    "    utilMat = np.array(train)\n",
    "\n",
    "    # the nan or unavailable entries are masked\n",
    "    mask = np.isnan(utilMat)\n",
    "    masked_arr = np.ma.masked_array(utilMat, mask)\n",
    "\n",
    "    item_means = np.mean(masked_arr, axis=0)\n",
    "\n",
    "    # nan entries will replaced by the average rating for each item\n",
    "    utilMat = masked_arr.filled(item_means)\n",
    "    x = np.tile(item_means, (utilMat.shape[0],1))\n",
    "\n",
    "    # we remove the per item average from all entries.\n",
    "    # the above mentioned nan entries will be essentially zero now\n",
    "    utilMat = utilMat - x\n",
    "\n",
    "    # The magic happens here. U and V are user and item features\n",
    "    U, s, V=np.linalg.svd(utilMat, full_matrices=False)\n",
    "    s=np.diag(s)\n",
    "\n",
    "    # we take only the k most significant features\n",
    "    s=s[0:k,0:k]\n",
    "    U=U[:,0:k]\n",
    "    V=V[0:k,:]\n",
    "\n",
    "    s_root=sqrtm(s)\n",
    "    Usk=np.dot(U,s_root)\n",
    "    skV=np.dot(s_root,V)\n",
    "    UsV = np.dot(Usk, skV)\n",
    "    UsV = UsV + x\n",
    "    return UsV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3sG_ANug_Sj_"
   },
   "outputs": [],
   "source": [
    "def rmse(true, pred):\n",
    "    x = true - pred\n",
    "    return sum([xi*xi for xi in x])/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zalPgxeI_Wh-"
   },
   "outputs": [],
   "source": [
    "uiMat, users_index, items_index = create_user_item_matrix(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixHyBKd3_q5f",
    "outputId": "f123df74-c648-4f27-ff56-5e9628d24459"
   },
   "outputs": [],
   "source": [
    "#svd evaluation\n",
    "for f in no_of_features:\n",
    "  svdout = svd(uiMat, k=f)\n",
    "  pred = [] #to store the predicted ratings\n",
    "  for _,row in test.iterrows():\n",
    "    user = row['userId']\n",
    "    item = row['movieId']\n",
    "    u_index = users_index[user]\n",
    "    \n",
    "    if item in items_index:\n",
    "      i_index = items_index[item]\n",
    "      pred_rating = svdout[u_index, i_index]\n",
    "    else:\n",
    "      pred_rating = np.mean(svdout[u_index, :])\n",
    "    pred.append(pred_rating)\n",
    "  print(rmse(test['rating'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYdr_ohfTZwz"
   },
   "outputs": [],
   "source": [
    "def filter(u_id, user_genres = ds_user_genres, item_genres = ds_movies[\"genres\"]):\n",
    "  u_likes = user_genres[u_id][\"like\"]\n",
    "  u_detest = user_genres[u_id][\"dislike\"]\n",
    "\n",
    "  useritemMat, users_index, items_index = create_user_item_matrix(ds_ratings)\n",
    "  svdout = svd(useritemMat, k=12)\n",
    "  user_pos = users_index[str(u_id)]\n",
    "  result = []\n",
    "  for i in range(len(svdout[user_pos])):\n",
    "\n",
    "    if str(i) in items_index:\n",
    "      i_id = items_index[str(i)]\n",
    "      i_genre = item_genres[i_id].split(sep=\"|\")\n",
    "\n",
    "      checkDislike =  any(item in i_genre for item in u_detest)\n",
    "      if checkDislike is False:\n",
    "        new_rate = svdout[user_pos][i]\n",
    "\n",
    "        checkLike =  any(item in i_genre for item in u_likes)\n",
    "        if checkLike is True:\n",
    "          new_rate = new_rate + 2\n",
    "        result.append([ds_movies['movieId'][i_id], ds_movies[\"title\"][i_id], new_rate])\n",
    " \n",
    "  return sorted(result, key=lambda item: item[2], reverse=True)[:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7JekM73eErX"
   },
   "outputs": [],
   "source": [
    "check =  any(item in [\"Horror\", \"Comedy\"] for item in [\"Comedy\", \"Sci-Fi\", \"Mystery\", \"Thriller\"])\n",
    "if check is True:\n",
    "    print(\"pass\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ilIi5vZJbgd6"
   },
   "outputs": [],
   "source": [
    "filter(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7wSplTSCq7_"
   },
   "source": [
    "**Content-based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INdMx6n9Cs_P"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, np.transpose(v)) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Yf5cIo8rOe1"
   },
   "outputs": [],
   "source": [
    "def readPlots(movies):\n",
    "  plots = []\n",
    "  for movie in movies:\n",
    "    url = 'https://zrekoj.github.io/hybrid-recommender-system/movies/'+ str(movie)+'.json'\n",
    "    archivo = pd.read_json(url)\n",
    "    plots.append([archivo['Plot'].tolist()[0]])\n",
    "  return plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqRoOqAubo6O"
   },
   "source": [
    "Build Frecuency Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNUIYzosbxbh"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(doc):\n",
    " stopset = set(stopwords.words('english'))\n",
    " stemmer = SnowballStemmer('english',ignore_stopwords=True)\n",
    " tokens = wordpunct_tokenize(doc)\n",
    " clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    " stemmed_text = [stemmer.stem(word) for word in clean]\n",
    " return stemmed_text\n",
    " \n",
    " \n",
    "def create_freq_dict(movieIds, preprocessed_texts):\n",
    " i=0\n",
    " print(\"Creating frequency dictionary\")\n",
    " freqDict_list=[]\n",
    " all_tokens=set()\n",
    " for tokens in preprocessed_texts:\n",
    "  freq_dict={}\n",
    "  \n",
    "  for token in tokens:\n",
    "    if token in freq_dict:\n",
    "     freq_dict[token]+=1\n",
    "    else:\n",
    "     freq_dict[token]=1\n",
    "\n",
    "  temp={'term_id':i,'movie_id':movieIds[i],'freq_dict':freq_dict,'doc_length':len(tokens)}\n",
    "  i+=1\n",
    "  freqDict_list.append(temp)\n",
    " return freqDict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ4w4ThMbyiK"
   },
   "source": [
    "Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxnJAY9xiQA2"
   },
   "source": [
    "Content-Based application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdp1gOGyb3kB"
   },
   "outputs": [],
   "source": [
    "def computeTF(freqDict_list):\n",
    " TF_scores=[]\n",
    " print(\"Calculating TF\")\n",
    " for tempDict in freqDict_list:\n",
    "  id=tempDict['term_id']\n",
    "  name=tempDict['movie_id']\n",
    "  for k in tempDict['freq_dict']:\n",
    "   temp={'term_id':id, 'movie_id':name,'TF_score':tempDict['freq_dict'][k]/tempDict['doc_length'],'key':k}\n",
    "   TF_scores.append(temp)\n",
    " return TF_scores\n",
    " \n",
    "def computeIDF(freqDict_list):\n",
    "  IDF_scores=[]\n",
    "  print(\"Calculating IDF\")\n",
    "  counter=-1\n",
    "  for dict in freqDict_list:\n",
    "   counter+=1\n",
    "   for k in dict['freq_dict'].keys():\n",
    "    count=sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "    temp= {'term_id': counter, 'IDF_score':math.log(1+(dict['doc_length']/count)),'key':k}\n",
    "    IDF_scores.append(temp)\n",
    "  return IDF_scores\n",
    " \n",
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    " TF_IDF_scores=[]\n",
    " print(\"Calculating TF-IDF\")\n",
    " \n",
    " for j in IDF_scores:\n",
    "  for i in TF_scores:\n",
    "   if j['key']==i['key'] and j['term_id']==i['term_id']:\n",
    "    temp={'term_id':j['term_id'], 'movie_id':i['movie_id'], 'TFIDF_score': j['IDF_score']*i['TF_score'], 'key':i['key']}\n",
    "    TF_IDF_scores.append(temp)\n",
    " return TF_IDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFknV71aVRK6",
    "outputId": "b03ae66a-e902-4c96-a2f9-69d1ec9d4ac0"
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#zihao\n",
    "\n",
    "import requests \n",
    "\n",
    "user_id = 1\n",
    "hybrid = False\n",
    "\n",
    "movie_similarities=[]\n",
    "\n",
    "#Get imbdIds of user movies\n",
    "user_movies = list(\n",
    "    map(\n",
    "        lambda movie_id : str(ds_links.loc[ds_links['movieId'] == str(movie_id), 'imdbId'].values[0]), \n",
    "        ds_ratings.loc[ds_ratings['userId'] == str(user_id), 'movieId'].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "#collaborative_filtering_result = filter(user_id)\n",
    "movies=[]\n",
    "\n",
    "#If we are using content-based as part of the hybrid filter, it uses the results of the collaborative filtering\n",
    "#Otherwise it uses all movies\n",
    "if(hybrid):\n",
    "    movies = list(\n",
    "        map(\n",
    "            lambda result : ds_links.loc[ds_links['movieId'] == str(result[0]), 'imdbId'].values[0], \n",
    "            collaborative_filtering_result\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    movies=[str(movie_id) for movie_id in list_movies_imdbid if movie_id not in user_movies]\n",
    "    \n",
    "response = requests.get('https://zrekoj.github.io/hybrid-recommender-system/user_frequency/'+ str(user_id)+'.json') \n",
    "user_frequence_dict = response.json()\n",
    "\n",
    "def computeTFIDF(user_frequence_dict):\n",
    "    copied = copy.deepcopy(user_frequence_dict)\n",
    "    for frequency in copied.values():\n",
    "        length = len(frequency)\n",
    "        total_count = sum(frequency.values())\n",
    "        for key in frequency:\n",
    "            frequency[key] = (frequency[key] / length) * (math.log(1+(length / total_count)))\n",
    "    return copied\n",
    "            \n",
    "for movie in movies:\n",
    "    MaxSim = 0\n",
    "    \n",
    "    movie_frequence_dict = requests.get('https://zrekoj.github.io/hybrid-recommender-system/frequency/'+ str(movie)+'.json').json() \n",
    "    user_frequence_dict[str(movie)] = movie_frequence_dict\n",
    "    TFIDF = computeTFIDF(user_frequence_dict)\n",
    "    user_frequence_dict.pop(str(movie), None)\n",
    "    \n",
    "    TFIDF_highest = TFIDF[str(movie)]\n",
    "    for user_movie in user_movies[:10]:\n",
    "        TFIDF_user =  TFIDF[str(user_movie)]\n",
    "        TFIDF_user_values = list(\n",
    "            map(\n",
    "                lambda key : TFIDF_user[key] if key in TFIDF_user.keys() else 0,\n",
    "                TFIDF_highest.keys()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        similarity = cosine_similarity(list(TFIDF_highest.values()), TFIDF_user_values)        \n",
    "        \n",
    "        if(similarity > MaxSim):\n",
    "            MaxSim = similarity\n",
    "    movie_similarities.append([movie, MaxSim])\n",
    "    print(movie, MaxSim)\n",
    "\n",
    "sorted(movie_similarities, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(movie_similarities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvvBWUOud1mz",
    "outputId": "d63f055f-3fd0-4e4a-979c-be80feee7849"
   },
   "outputs": [],
   "source": [
    "def content_based(user_id, hybrid=True):\n",
    "\n",
    "  movie_similarities=[]\n",
    "  temp_user_movies=[]\n",
    "  temp_movies=[]\n",
    "  user_movies=[]\n",
    "  #Get movies of selected user\n",
    "  for k in range(0,len(ds_ratings)-1):\n",
    "    if str(ds_ratings['userId'][k])==str(user_id):\n",
    "      temp_user_movies.append(ds_ratings['movieId'][k])\n",
    "  #Get imbdIds of user movies\n",
    "  for k in range(0,len(ds_links)-1):\n",
    "    if str(ds_links['movieId'][k]) in temp_user_movies:\n",
    "      user_movies.append(str(ds_links['imdbId'][k]))\n",
    "  \n",
    "  movies=[]\n",
    "  \n",
    "  #If we are using content-based as part of the hybrid filter, it uses the results of the collaborative filtering\n",
    "  #Otherwise it uses all movies\n",
    "  if(hybrid):\n",
    "    temp_movies=[str(x[0]) for x in filter(user_id)]\n",
    "    for k in range(0,len(ds_links)):\n",
    "      if str(ds_links['movieId'][k]) in temp_movies:\n",
    "        movies.append(str(ds_links['imdbId'][k]))\n",
    "  else:\n",
    "    movies=[str(movie_id) for movie_id in list_movies_imdbid if movie_id not in user_movies]\n",
    "  #Preprocess user movies\n",
    "  preprocessed_texts=[preprocess_text(text[0]) for text in readPlots(user_movies)]\n",
    "\n",
    "  for i in range(0,len(movies)-1):\n",
    "    MaxSim=0\n",
    "    user_movies.append(movies[i])\n",
    "    preprocessed_texts.append(preprocess_text(readPlots([movies[i]])[0][0]))\n",
    "    freqDict=create_freq_dict(user_movies, preprocessed_texts)\n",
    "    user_movies.pop(-1)\n",
    "    preprocessed_texts.pop(-1)\n",
    "    TFIDF=computeTFIDF(computeTF(freqDict),computeIDF(freqDict))\n",
    "\n",
    "    TFIDF_highest_temp=[[y[\"key\"],y[\"TFIDF_score\"]] for y in TFIDF if y[\"movie_id\"]==movies[i]]\n",
    "    TFIDF_highest=[y[1] for y in TFIDF_highest_temp]\n",
    "    \n",
    "    for j in range(0,9):\n",
    "      \n",
    "      TFIDF_user_temp={x[\"key\"]:x[\"TFIDF_score\"] for x in TFIDF if x[\"movie_id\"]==user_movies[j]}\n",
    "      TFIDF_user=[]\n",
    "      extra_values=[]\n",
    "    \n",
    "      for x in TFIDF_highest_temp:\n",
    "        if x[0] in TFIDF_user_temp.keys():\n",
    "          TFIDF_user.append(TFIDF_user_temp[x[0]])\n",
    "        else:\n",
    "          TFIDF_user.append(0)\n",
    "          extra_values.append(x[0])\n",
    "\n",
    "      \n",
    "      similarity=cosine_similarity(TFIDF_highest+extra_values,TFIDF_user+[0]*len(extra_values))\n",
    "      if(similarity>MaxSim):\n",
    "        MaxSim=similarity\n",
    "    movie_similarities.append([movies[i],MaxSim])\n",
    "  return sorted(movie_similarities, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(content_based(1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h66pQrMvx3oC"
   },
   "source": [
    "Fuzzy Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "AooHYk2M_pY4",
    "outputId": "53624136-87cb-487c-9607-a361ea979263"
   },
   "outputs": [],
   "source": [
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4V7Sd0pyT-O"
   },
   "outputs": [],
   "source": [
    "inp1 = ctrl.Antecedent(np.arange(0, 5, 1), 'average_rating')\n",
    "inp2 = ctrl.Antecedent(np.arange(0, 350, 1), 'total_ratings')\n",
    "inp3 = ctrl.Antecedent(np.arange(0, 1, 1), 'similarity')\n",
    "out = ctrl.Consequent(np.arange(0, 1, 1), 'importance')\n",
    "\n",
    "out['low'] = fuzz.trimf(out.universe, [0, 0, .2])\n",
    "out['medium'] = fuzz.trimf(out.universe, [0, .6, 25])\n",
    "out['high'] = fuzz.trimf(out.universe, [.6, 1, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "recommender.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLNqYBgmGIyf"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9hzzVAc5GBA_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "import threading\n",
    "import copy\n",
    "import requests \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize.regexp import (WordPunctTokenizer,wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vH15YyQGM1I"
   },
   "source": [
    "Links for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Kj91r1mXGRPD"
   },
   "outputs": [],
   "source": [
    "url_links = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/links.csv'\n",
    "url_movies = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/movies.csv'\n",
    "url_ratings = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/ratings.csv'\n",
    "url_tags = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/tags.csv'\n",
    "url_genres = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/user_genre.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7uqFvueudeU"
   },
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JUqBO0kkucLC"
   },
   "outputs": [],
   "source": [
    "ds_links = pd.read_csv(url_links, dtype = str)\n",
    "ds_movies = pd.read_csv(url_movies, dtype = str)\n",
    "ds_ratings = pd.read_csv(url_ratings)\n",
    "ds_tags = pd.read_csv(url_tags, dtype = str)\n",
    "ds_user_genres = pd.read_json(url_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uxrT24702OZK"
   },
   "outputs": [],
   "source": [
    "list_movies_id = list(ds_movies['movieId'].unique())\n",
    "list_title_id = ds_movies['title'].tolist()\n",
    "list_users_id = list(ds_ratings['userId'].unique())\n",
    "list_movies_imdbid = ds_links['imdbId'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfYUIC48Cta9"
   },
   "source": [
    "Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zY9Fa_VcLVa"
   },
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "szoO5K7IKg5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Thriller', 'Sci-Fi', 'Adventure', 'Comedy'],\n",
       " ['War', 'Romance', 'Action', 'IMAX'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getGenresByUser(user_id, ds=ds_user_genres):\n",
    "  return ds[user_id]['like'], ds[user_id]['dislike']\n",
    "getGenresByUser(616)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KRrF5bPD_bZs"
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "\n",
    "test_ratio = 0.2 #fraction of data to be used as test set.\n",
    "no_of_features = [8,10,12,14,17,20] # to test the performance over a different number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HVL4pD2A9zIu"
   },
   "outputs": [],
   "source": [
    "ds_ratings['userId'] = ds_ratings['userId'].astype('str')\n",
    "ds_ratings['movieId'] = ds_ratings['movieId'].astype('str')\n",
    "\n",
    "users = ds_ratings['userId'].unique() #list of all users\n",
    "movies = ds_ratings['movieId'].unique() #list of all movies\n",
    "\n",
    "test = pd.DataFrame(columns=ds_ratings.columns)\n",
    "train = pd.DataFrame(columns=ds_ratings.columns)\n",
    "\n",
    "for u in users:\n",
    "  temp = ds_ratings[ds_ratings['userId'] == u]\n",
    "  n = len(temp)\n",
    "  test_size = int(test_ratio*n)\n",
    "\n",
    "  temp = temp.sort_values('timestamp').reset_index()\n",
    "  temp.drop('index', axis=1, inplace=True)\n",
    "\n",
    "  dummy_test = temp.loc[n-1-test_size :]\n",
    "  dummy_train = temp.loc[: n-2-test_size]\n",
    "    \n",
    "  test = pd.concat([test, dummy_test])\n",
    "  train = pd.concat([train, dummy_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "GvRLP6__-98r"
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def create_user_item_matrix(data, formatizer = {'user':0, 'item': 1, 'value': 2}):\n",
    "    itemField = formatizer['item']\n",
    "    userField = formatizer['user']\n",
    "    valueField = formatizer['value']\n",
    "\n",
    "    \n",
    "    userList = data.iloc[:,userField].tolist()\n",
    "    itemList = data.iloc[:,itemField].tolist()\n",
    "    valueList = data.iloc[:,valueField].tolist()\n",
    "\n",
    "    users = list(set(data.iloc[:,userField]))\n",
    "    items = list(set(data.iloc[:,itemField]))\n",
    "\n",
    "    users_index = {users[i]: i for i in range(len(users))}\n",
    "\n",
    "    pd_dict = {item: [np.nan for i in range(len(users))] for item in items}\n",
    "\n",
    "    for i in range(0,len(data)):\n",
    "      item = itemList[i]\n",
    "      user = userList[i]\n",
    "      value = valueList[i]\n",
    "\n",
    "      pd_dict[item][users_index[user]] = value\n",
    "    \n",
    "    X = pd.DataFrame(pd_dict)\n",
    "    X.index = users\n",
    "\n",
    "    itemcols = list(X.columns)\n",
    "    items_index = {itemcols[i]: i for i in range(len(itemcols))}\n",
    "\n",
    "    # users_index gives us a mapping of user_id to index of user\n",
    "    # items_index provides the same for items\n",
    "\n",
    "    return X, users_index, items_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tr-QPs9A_K6a"
   },
   "outputs": [],
   "source": [
    "def svd(ratings, k, eval=False):\n",
    "    utilMat = np.array(ratings)\n",
    "\n",
    "    # the nan or unavailable entries are masked\n",
    "    mask = np.isnan(utilMat)\n",
    "    masked_arr = np.ma.masked_array(utilMat, mask)\n",
    "\n",
    "    item_means = np.mean(masked_arr, axis=0)\n",
    "\n",
    "    # nan entries will replaced by the average rating for each item\n",
    "    utilMat = masked_arr.filled(item_means)\n",
    "    x = np.tile(item_means, (utilMat.shape[0],1))\n",
    "\n",
    "    # we remove the per item average from all entries.\n",
    "    # the above mentioned nan entries will be essentially zero now\n",
    "    utilMat = utilMat - x\n",
    "\n",
    "    # The magic happens here. U and V are user and item features\n",
    "    U, s, V=np.linalg.svd(utilMat, full_matrices=False)\n",
    "    s=np.diag(s)\n",
    "\n",
    "    # we take only the k most significant features\n",
    "    s=s[0:k,0:k]\n",
    "    U=U[:,0:k]\n",
    "    V=V[0:k,:]\n",
    "\n",
    "    s_root=sqrtm(s)\n",
    "    Usk=np.dot(U,s_root)\n",
    "    skV=np.dot(s_root,V)\n",
    "    UsV = np.dot(Usk, skV)\n",
    "\n",
    "    if eval is True:\n",
    "        UsV = UsV + x\n",
    "    return UsV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3sG_ANug_Sj_"
   },
   "outputs": [],
   "source": [
    "def rmse(true, pred):\n",
    "    x = true - pred\n",
    "    return sum([xi*xi for xi in x])/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zalPgxeI_Wh-"
   },
   "outputs": [],
   "source": [
    "uiMat, users_index, items_index = create_user_item_matrix(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixHyBKd3_q5f",
    "outputId": "f123df74-c648-4f27-ff56-5e9628d24459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0049022470868392\n",
      "1.004761467417924\n",
      "1.005846364812009\n",
      "1.00652934602098\n",
      "1.0057963138575086\n",
      "1.0067926871975066\n"
     ]
    }
   ],
   "source": [
    "#svd evaluation\n",
    "for f in no_of_features:\n",
    "  svdout = svd(uiMat, k=f, eval=True)\n",
    "  pred = [] #to store the predicted ratings\n",
    "  for _,row in test.iterrows():\n",
    "    user = row['userId']\n",
    "    item = row['movieId']\n",
    "    u_index = users_index[user]\n",
    "    \n",
    "    if item in items_index:\n",
    "      i_index = items_index[item]\n",
    "      pred_rating = svdout[u_index, i_index]\n",
    "    else:\n",
    "      pred_rating = np.mean(svdout[u_index, :])\n",
    "    pred.append(pred_rating)\n",
    "  print(rmse(test['rating'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "useritemMat, u_index, i_index = create_user_item_matrix(ds_ratings)\n",
    "svdout = svd(useritemMat, k=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yYdr_ohfTZwz"
   },
   "outputs": [],
   "source": [
    "def collaborative_filtering(u_id, svdout=svdout, user_genres = ds_user_genres, movies = ds_movies, u_index=u_index, i_index=i_index):\n",
    "  user_pos = u_index[str(u_id)]\n",
    "  u_likes, u_detest = getGenresByUser(u_id)\n",
    "\n",
    "  result = []\n",
    "  for i_id in i_index:\n",
    "    item_pos = i_index[i_id]\n",
    "    item_row = movies.loc[movies['movieId'] == str(i_id)]\n",
    "    i_genre = item_row[\"genres\"].tolist()[0].split(sep=\"|\")\n",
    "    checkDislike =  any(item in i_genre for item in u_detest)\n",
    "    \n",
    "    if checkDislike is False:\n",
    "      new_rate = svdout[user_pos, item_pos]\n",
    "      checkLike = any(item in i_genre for item in u_likes)\n",
    "      \n",
    "      if checkLike is True:\n",
    "        new_rate = new_rate + new_rate * .3\n",
    "      else:\n",
    "        new_rate = new_rate - new_rate * .2\n",
    "      if (new_rate > 5.0):\n",
    "        new_rate = 5.0\n",
    "\n",
    "      result.append([i_id, item_row[\"title\"].tolist()[0], new_rate])\n",
    "  \n",
    "  return sorted(result, key=lambda item: item[2], reverse=True)[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7wSplTSCq7_"
   },
   "source": [
    "**Content-based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "INdMx6n9Cs_P"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, np.transpose(v)) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ4w4ThMbyiK"
   },
   "source": [
    "Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jdp1gOGyb3kB"
   },
   "outputs": [],
   "source": [
    " def computeTFIDF(user_frequence_dict):\n",
    "        copied = copy.deepcopy(user_frequence_dict)\n",
    "        total_movies = len(copied)\n",
    "        total_word_movies_count = {}\n",
    "\n",
    "        for frequency in copied.values():\n",
    "            for word in frequency:\n",
    "                total_word_movies_count[word] = total_word_movies_count.get(word, 0) + 1\n",
    "\n",
    "        for frequency in copied.values():\n",
    "            total_words_count = len(frequency)\n",
    "            for word in frequency:\n",
    "                #                TF                                 IDF\n",
    "                frequency[word] = (frequency[word] / total_words_count) * (math.log(1 + (total_movies / total_word_movies_count[word])))\n",
    "        return copied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxnJAY9xiQA2"
   },
   "source": [
    "Content-Based application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def content_based(user_id, hybrid=True):\n",
    "\n",
    "    movie_similarities=[]\n",
    "\n",
    "    #Get imbdIds of user movies\n",
    "    user_movies = list(\n",
    "        map(\n",
    "            lambda movie_id : str(ds_links.loc[ds_links['movieId'] == str(movie_id), 'imdbId'].values[0]), \n",
    "            ds_ratings.loc[ds_ratings['userId'] == str(user_id), 'movieId'].tolist()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    movies=[]\n",
    "\n",
    "    #If we are using content-based as part of the hybrid filter, it uses the results of the collaborative filtering\n",
    "    #Otherwise it uses all movies\n",
    "    if(hybrid):\n",
    "        collaborative_filtering_result = collaborative_filtering(user_id)\n",
    "        movies = list(\n",
    "            map(\n",
    "                lambda result : ds_links.loc[ds_links['movieId'] == str(result[0]), 'imdbId'].values[0], \n",
    "                collaborative_filtering_result\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        movies=[str(movie_id) for movie_id in list_movies_imdbid if movie_id not in user_movies]\n",
    "    response = requests.get('https://zrekoj.github.io/hybrid-recommender-system/user_frequency/'+ str(user_id)+'.json') \n",
    "    user_frequence_dict = response.json()\n",
    "    i=0\n",
    "    for movie in movies:\n",
    "        MaxSim = 0\n",
    "\n",
    "        movie_frequence_dict = requests.get('https://zrekoj.github.io/hybrid-recommender-system/frequency/'+ str(movie)+'.json').json() \n",
    "        user_frequence_dict[str(movie)] = movie_frequence_dict\n",
    "        TFIDF = computeTFIDF(user_frequence_dict)\n",
    "        if(movie not in user_movies):\n",
    "            user_frequence_dict.pop(str(movie), None)\n",
    "\n",
    "        no_exist_string = \"***###***\"\n",
    "        \n",
    "        TFIDF_highest = TFIDF[str(movie)]\n",
    "        \n",
    "        for user_movie in user_movies:\n",
    "            TFIDF_user =  TFIDF[str(user_movie)]\n",
    "            \n",
    "            TFIDF_highest_keys = TFIDF_highest.keys()\n",
    "            TFIDF_user_keys = TFIDF_user.keys()\n",
    "            \n",
    "            coincidence = list(set(TFIDF_highest_keys) & set(TFIDF_user_keys))\n",
    "            TFIDF_highest_values_diff = list(set(TFIDF_highest_keys) - set(TFIDF_user_keys))\n",
    "            TFIDF_user_values_diff = list(set(TFIDF_user_keys) - set(TFIDF_highest_keys))\n",
    "            \n",
    "            TFIDF_highest_values = coincidence + TFIDF_highest_values_diff + ([no_exist_string] * len(TFIDF_user_values_diff))\n",
    "            TFIDF_user_values = coincidence + ([no_exist_string] * len(TFIDF_highest_values_diff)) + TFIDF_user_values_diff\n",
    "            \n",
    "            TFIDF_highest_values = list(\n",
    "                map(\n",
    "                    lambda value : TFIDF_highest[value] if value != no_exist_string else 0,\n",
    "                    TFIDF_highest_values\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            TFIDF_user_values = list(\n",
    "                map(\n",
    "                    lambda value : TFIDF_user[value] if value != no_exist_string else 0,\n",
    "                    TFIDF_user_values\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            similarity = cosine_similarity(TFIDF_highest_values, TFIDF_user_values)        \n",
    "\n",
    "            if(similarity > MaxSim):\n",
    "                MaxSim = similarity\n",
    "        if(hybrid):\n",
    "            movie_similarities.append([movie, MaxSim,collaborative_filtering_result[i][0],collaborative_filtering_result[i][1],collaborative_filtering_result[i][2]])\n",
    "            \n",
    "        else:\n",
    "            movie_similarities.append([movie, MaxSim])\n",
    "        i+=1\n",
    "        \n",
    "\n",
    "    return sorted(movie_similarities, key=lambda item: item[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h66pQrMvx3oC"
   },
   "source": [
    "Fuzzy Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "AooHYk2M_pY4",
    "outputId": "53624136-87cb-487c-9607-a361ea979263"
   },
   "outputs": [],
   "source": [
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized\n",
    "\n",
    "def fuzzy(user_id, visualize=False):\n",
    "    content_based_result=content_based(user_id)\n",
    "\n",
    "\n",
    "\n",
    "    #Generate variables\n",
    "    inp1 = np.arange(0, 1.01, 0.01)\n",
    "    inp2 = np.arange(0, 1.01, 0.01)\n",
    "    inp3 = np.arange(0, 1.01, 0.01)\n",
    "    importance = np.arange(0, 1.01, 0.01)\n",
    "\n",
    "    #Generate fuzzy membership functions\n",
    "    inp1_low = fuzz.trimf(inp1, [0, 0, 0.5])\n",
    "    inp1_medium = fuzz.trimf(inp1, [0, 0.5, 1])\n",
    "    inp1_high = fuzz.trimf(inp1, [0.5, 1, 1])\n",
    "    inp2_few = fuzz.trimf(inp2, [0, 0, 0.3])\n",
    "    inp2_medium = fuzz.trimf(inp2, [0.2, 0.4, 0.6])\n",
    "    inp2_much = fuzz.trimf(inp2, [0.4, 0.6, 0.8])\n",
    "    inp2_very_much = fuzz.trimf(inp2, [0.7, 1, 1])\n",
    "    inp3_very_low = fuzz.trimf(inp3, [0, 0, 0.06])\n",
    "    inp3_low = fuzz.trimf(inp3, [0, 0, 0.06])\n",
    "    inp3_medium = fuzz.trimf(inp3, [0.05, 0.07, 0.09])\n",
    "    inp3_high = fuzz.trimf(inp3, [0.07, 0.1, 0.13])\n",
    "    inp3_very_high = fuzz.trimf(inp3, [0.1, 1, 1])\n",
    "\n",
    "    importance_very_low = fuzz.trimf(inp1, [0, 0, 0.25])\n",
    "    importance_low = fuzz.trimf(inp1, [0, 0.25, 0.5])\n",
    "    importance_medium = fuzz.trimf(inp1, [0.25, 0.5, 0.75])\n",
    "    importance_high = fuzz.trimf(inp1, [0.5, 0.75, 1])\n",
    "    importance_very_high = fuzz.trimf(inp1, [0.75, 1, 1])\n",
    "\n",
    "    #Visualize fuzzy membership functions\n",
    "    fig, (ax0, ax1, ax2) = plt.subplots(nrows=3, figsize=(8, 9))\n",
    "    ax0.plot(inp1, inp1_low, 'b', linewidth=1.5, label='Low')\n",
    "    ax0.plot(inp1, inp1_medium, 'g', linewidth=1.5, label='Medium')\n",
    "    ax0.plot(inp1, inp1_high, 'r', linewidth=1.5, label='High')\n",
    "    ax0.set_title('Average rating')\n",
    "    ax0.legend()\n",
    "\n",
    "    ax1.plot(inp2, inp2_few, 'b', linewidth=1.5, label='Few')\n",
    "    ax1.plot(inp2, inp2_medium, 'g', linewidth=1.5, label='Medium')\n",
    "    ax1.plot(inp2, inp2_much, 'r', linewidth=1.5, label='Much')\n",
    "    ax1.plot(inp2, inp2_very_much, 'b', linewidth=1.5, label='Very much')\n",
    "    ax1.set_title('Total ratings')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(inp3, inp3_very_low, 'b', linewidth=1.5, label='Very low')\n",
    "    ax2.plot(inp3, inp3_low, 'g', linewidth=1.5, label='Low')\n",
    "    ax2.plot(inp3, inp3_medium, 'r', linewidth=1.5, label='Medium')\n",
    "    ax2.plot(inp3, inp3_high, 'b', linewidth=1.5, label='High')\n",
    "    ax2.plot(inp3, inp3_very_high, 'g', linewidth=1.5, label='Very high')\n",
    "    ax2.set_title('Similarity')\n",
    "    ax2.legend()\n",
    "    result=[]\n",
    "    for i in range(0,len(content_based_result)):\n",
    "        movie_ratings = ds_ratings[ds_ratings['movieId']==str(content_based_result[i][3])]\n",
    "        average_rating=np.average(movie_ratings['rating'])/5\n",
    "        total_rating=len(movie_ratings['rating'])/350\n",
    "        #Apply rules\n",
    "        inp1_lo = fuzz.interp_membership(inp1, inp1_low, average_rating)\n",
    "        inp1_md = fuzz.interp_membership(inp1, inp1_medium, average_rating)\n",
    "        inp1_hi = fuzz.interp_membership(inp1, inp1_high, average_rating)\n",
    "        inp2_fe = fuzz.interp_membership(inp2, inp2_few, total_rating)\n",
    "        inp2_md = fuzz.interp_membership(inp2, inp2_medium, total_rating)\n",
    "        inp2_mu = fuzz.interp_membership(inp2, inp2_much, total_rating)\n",
    "        inp2_vm = fuzz.interp_membership(inp2, inp2_very_much, total_rating)\n",
    "        inp3_lo = fuzz.interp_membership(inp3, inp3_low, content_based_result[i][1])\n",
    "        inp3_md = fuzz.interp_membership(inp3, inp3_medium, content_based_result[i][1])\n",
    "        inp3_hi = fuzz.interp_membership(inp3, inp3_high, content_based_result[i][1])\n",
    "        inp3_vh = fuzz.interp_membership(inp3, inp3_very_high, content_based_result[i][1])\n",
    "\n",
    "        inp1_low_and_inp2_few = np.fmin(inp1_lo, inp2_fe)\n",
    "        inp1_low_and_inp2_medium = np.fmin(inp1_lo, inp2_md)\n",
    "        inp1_low_and_inp2_much = np.fmin(inp1_lo, inp2_mu)\n",
    "        inp1_low_and_inp2_very_much = np.fmin(inp1_lo, inp2_vm)\n",
    "\n",
    "        inp1_medium_and_inp2_few = np.fmin(inp1_md, inp2_fe)\n",
    "        inp1_medium_and_inp2_medium = np.fmin(inp1_md, inp2_md)\n",
    "        inp1_medium_and_inp2_much = np.fmin(inp1_md, inp2_mu)\n",
    "        inp1_medium_and_inp2_very_much = np.fmin(inp1_md, inp2_vm)\n",
    "\n",
    "        inp1_high_and_inp3_low = np.fmin(inp1_hi, inp3_lo)\n",
    "        inp1_high_and_inp3_med = np.fmin(inp1_hi, inp3_md)\n",
    "        inp1_high_and_inp3_high = np.fmin(inp1_hi, inp3_hi)\n",
    "        inp1_high_and_inp3_very_high = np.fmin(inp1_hi, inp3_vh)\n",
    "\n",
    "        rule_001_005_009 = np.fmin(inp1_low_and_inp2_few, inp3_lo)\n",
    "        rule_002_006_010 = np.fmin(inp1_low_and_inp2_few, inp3_md)\n",
    "        rule_003_007_011 = np.fmin(inp1_low_and_inp2_few, inp3_hi)\n",
    "        rule_004_008_012 = np.fmin(inp1_low_and_inp2_few, inp3_vh)\n",
    "\n",
    "        rule_013_017_021 = np.fmin(inp1_low_and_inp2_medium, inp3_lo)\n",
    "        rule_014_018_022 = np.fmin(inp1_low_and_inp2_medium, inp3_md)\n",
    "        rule_015_019_023 = np.fmin(inp1_low_and_inp2_medium, inp3_hi)\n",
    "        rule_016_020_024 = np.fmin(inp1_low_and_inp2_medium, inp3_vh)\n",
    "\n",
    "        rule_025_029_033 = np.fmin(inp1_low_and_inp2_much, inp3_lo)\n",
    "        rule_026_030_034 = np.fmin(inp1_low_and_inp2_much, inp3_md)\n",
    "        rule_027_031_035 = np.fmin(inp1_low_and_inp2_much, inp3_hi)\n",
    "        rule_028_032_036 = np.fmin(inp1_low_and_inp2_much, inp3_vh)\n",
    "\n",
    "        rule_037_041_045 = np.fmin(inp1_low_and_inp2_very_much, inp3_lo)\n",
    "        rule_038_042_046 = np.fmin(inp1_low_and_inp2_very_much, inp3_md)\n",
    "        rule_039_043_047 = np.fmin(inp1_low_and_inp2_very_much, inp3_hi)\n",
    "        rule_040_044_048 = np.fmin(inp1_low_and_inp2_very_much, inp3_vh)\n",
    "\n",
    "        rule_049_053_057 = np.fmin(inp1_medium_and_inp2_few, inp3_lo)\n",
    "        rule_050_054_058 = np.fmin(inp1_medium_and_inp2_few, inp3_md)\n",
    "        rule_051_055_059 = np.fmin(inp1_medium_and_inp2_few, inp3_hi)\n",
    "        rule_052_056_060 = np.fmin(inp1_medium_and_inp2_few, inp3_vh)\n",
    "\n",
    "        rule_061_065_069 = np.fmin(inp1_medium_and_inp2_medium, inp3_lo)\n",
    "        rule_062_066_070 = np.fmin(inp1_medium_and_inp2_medium, inp3_md)\n",
    "        rule_063_067_071 = np.fmin(inp1_medium_and_inp2_medium, inp3_hi)\n",
    "        rule_064_068_072 = np.fmin(inp1_medium_and_inp2_medium, inp3_vh)\n",
    "\n",
    "        rule_073_077_081 = np.fmin(inp1_medium_and_inp2_much, inp3_lo)\n",
    "        rule_074_078_082 = np.fmin(inp1_medium_and_inp2_much, inp3_md)\n",
    "        rule_075_079_083 = np.fmin(inp1_medium_and_inp2_much, inp3_hi)\n",
    "        rule_076_080_084 = np.fmin(inp1_medium_and_inp2_much, inp3_vh)\n",
    "\n",
    "        rule_085_089_093 = np.fmin(inp1_medium_and_inp2_very_much, inp3_lo)\n",
    "        rule_086_090_094 = np.fmin(inp1_medium_and_inp2_very_much, inp3_md)\n",
    "        rule_087_091_095 = np.fmin(inp1_medium_and_inp2_very_much, inp3_hi)\n",
    "        rule_088_092_096 = np.fmin(inp1_medium_and_inp2_very_much, inp3_vh)\n",
    "\n",
    "        rule_097_101_105 = np.fmin(inp1_high_and_inp3_low, inp2_fe)\n",
    "        rule_098_102_106 = np.fmin(inp1_high_and_inp3_med, inp2_fe)\n",
    "        rule_099_103_107 = np.fmin(inp1_high_and_inp3_high, inp2_fe)\n",
    "        rule_100_104_108 = np.fmin(inp1_high_and_inp3_very_high, inp2_fe)\n",
    "\n",
    "        rule_109_113_117 = np.fmin(inp1_high_and_inp3_low, inp2_md)\n",
    "        rule_110_114_118 = np.fmin(inp1_high_and_inp3_med, inp2_md)\n",
    "        rule_111_115_119 = np.fmin(inp1_high_and_inp3_high, inp2_md)\n",
    "        rule_112_116_120 = np.fmin(inp1_high_and_inp3_very_high, inp2_md)\n",
    "\n",
    "        rule_121_125_129 = np.fmin(inp1_high_and_inp3_low, inp2_mu)\n",
    "        rule_122_126_130 = np.fmin(inp1_high_and_inp3_med, inp2_mu)\n",
    "        rule_123_127_131 = np.fmin(inp1_high_and_inp3_high, inp2_mu)\n",
    "        rule_124_128_132 = np.fmin(inp1_high_and_inp3_very_high, inp2_mu)\n",
    "\n",
    "        rule_133_137_141 = np.fmin(inp1_high_and_inp3_low, inp2_vm)\n",
    "        rule_134_138_142 = np.fmin(inp1_high_and_inp3_med, inp2_vm)\n",
    "        rule_135_139_143 = np.fmin(inp1_high_and_inp3_high, inp2_vm)\n",
    "        rule_136_140_144 = np.fmin(inp1_high_and_inp3_very_high, inp2_vm)\n",
    "\n",
    "\n",
    "        very_low_rules=np.fmax(rule_001_005_009,\n",
    "                               np.fmax(rule_002_006_010,\n",
    "                                       np.fmax(rule_003_007_011,\n",
    "                                               np.fmax(rule_004_008_012, \n",
    "                                                       np.fmax(rule_013_017_021 ,\n",
    "                                                               np.fmax(rule_014_018_022,\n",
    "                                                                       np.fmax(rule_015_019_023,\n",
    "                                                                               np.fmax(rule_016_020_024,\n",
    "                                                                                      np.fmax(rule_025_029_033,\n",
    "                                                                                             np.fmax(rule_026_030_034,\n",
    "                                                                                                    np.fmax(rule_027_031_035,\n",
    "                                                                                                           np.fmax(rule_028_032_036,\n",
    "                                                                                                                  np.fmax(rule_037_041_045,\n",
    "                                                                                                                         np.fmax(rule_038_042_046,\n",
    "                                                                                                                                np.fmax(rule_039_043_047,\n",
    "                                                                                                                                        np.fmax(rule_040_044_048,\n",
    "                                                                                                                                               np.fmax(rule_049_053_057 ,\n",
    "                                                                                                                                                      np.fmax(rule_050_054_058 ,rule_051_055_059 ))))))))))))))))))\n",
    "\n",
    "        low_rules=np.fmax(rule_004_008_012,\n",
    "        np.fmax(rule_049_053_057,\n",
    "        np.fmax(rule_050_054_058 ,\n",
    "        np.fmax(rule_051_055_059 ,\n",
    "        np.fmax(rule_052_056_060,\n",
    "        np.fmax(rule_061_065_069,\n",
    "        np.fmax(rule_062_066_070,\n",
    "        np.fmax(rule_063_067_071,\n",
    "        np.fmax(rule_064_068_072,\n",
    "        np.fmax(rule_073_077_081,\n",
    "        np.fmax(rule_074_078_082,\n",
    "        np.fmax(rule_075_079_083,\n",
    "        np.fmax(rule_085_089_093,\n",
    "        np.fmax(rule_086_090_094,\n",
    "        np.fmax(rule_097_101_105,rule_098_102_106)))))))))))))))\n",
    "\n",
    "        medium_rules=np.fmax(rule_052_056_060,\n",
    "                             np.fmax(rule_062_066_070,\n",
    "                                     np.fmax(rule_063_067_071,\n",
    "                                             np.fmax(rule_064_068_072,\n",
    "                                                     np.fmax(rule_073_077_081,\n",
    "                                                             np.fmax(rule_074_078_082,\n",
    "                                                                     np.fmax(rule_075_079_083,\n",
    "                                                                            np.fmax(rule_076_080_084,\n",
    "                                                                                    np.fmax(rule_085_089_093,\n",
    "                                                                                            np.fmax(rule_086_090_094, \n",
    "                                                                                                    np.fmax(rule_087_091_095,\n",
    "                                                                                                            np.fmax(rule_088_092_096,\n",
    "                                                                                                                np.fmax(rule_087_091_095,\n",
    "                                                                                                                        np.fmax(rule_088_092_096,\n",
    "                                                                                                                                np.fmax(rule_099_103_107,\n",
    "                                                                                                                                        np.fmax(rule_100_104_108, \n",
    "                                                                                                                                                np.fmax(rule_109_113_117,\n",
    "                                                                                                                                                    np.fmax(rule_110_114_118,        \n",
    "                                                                                                                                                        np.fmax(rule_111_115_119, \n",
    "                                                                                                                                                        np.fmax(rule_112_116_120,  \n",
    "                                                                                                                                                        np.fmax(rule_121_125_129,   \n",
    "                                                                                                                                                        np.fmax(rule_122_126_130,   \n",
    "                                                                                                                                                        np.fmax(rule_123_127_131, rule_133_137_141)))))))))))))))))))))))\n",
    "\n",
    "\n",
    "        high_rules = np.fmax(rule_100_104_108,\n",
    "                            np.fmax(rule_111_115_119, \n",
    "                                   np.fmax(rule_112_116_120, \n",
    "                                          np.fmax(rule_122_126_130, \n",
    "                                                 np.fmax(rule_123_127_131, \n",
    "                                                        np.fmax(rule_124_128_132, \n",
    "                                                               np.fmax(rule_133_137_141, \n",
    "                                                                      np.fmax(rule_134_138_142, \n",
    "                                                                             np.fmax(rule_135_139_143, rule_136_140_144)))))))))\n",
    "        very_high_rules = np.fmax(rule_124_128_132, np.fmax(rule_135_139_143, rule_136_140_144))\n",
    "\n",
    "        importance_activation_very_low = np.fmin(very_low_rules,importance_very_low)\n",
    "        importance_activation_low = np.fmin(low_rules,importance_low)\n",
    "        importance_activation_medium = np.fmin(medium_rules, importance_medium)\n",
    "        importance_activation_high = np.fmin(high_rules, importance_high)\n",
    "        importance_activation_very_high = np.fmin(very_high_rules, importance_very_high)\n",
    "        importance0 = np.zeros_like(importance)\n",
    "\n",
    "        fig, ax0 = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "        ax0.fill_between(importance, importance0, importance_activation_very_low, facecolor='b', alpha=0.7)\n",
    "        ax0.plot(importance, importance_very_low, 'b', linewidth=0.5, linestyle='--', )\n",
    "        ax0.fill_between(importance, importance0, importance_activation_low, facecolor='g', alpha=0.7)\n",
    "        ax0.plot(importance, importance_low, 'g', linewidth=0.5, linestyle='--')\n",
    "        ax0.fill_between(importance, importance0, importance_activation_medium, facecolor='r', alpha=0.7)\n",
    "        ax0.plot(importance, importance_medium, 'r', linewidth=0.5, linestyle='--')\n",
    "        ax0.fill_between(importance, importance0, importance_activation_high, facecolor='b', alpha=0.7)\n",
    "        ax0.plot(importance, importance_high, 'b', linewidth=0.5, linestyle='--', )\n",
    "        ax0.fill_between(importance, importance0, importance_activation_very_high, facecolor='g', alpha=0.7)\n",
    "        ax0.plot(importance, importance_very_high, 'g', linewidth=0.5, linestyle='--')\n",
    "        ax0.set_title('Output membership activity for '+content_based_result[i][3])\n",
    "\n",
    "        #Defuzzification\n",
    "        aggregated = np.fmax(importance_activation_very_low,np.fmax(importance_activation_low, np.fmax(importance_activation_medium,np.fmax(importance_activation_high, importance_activation_very_high))))\n",
    "        importance_defuzz = fuzz.defuzz(importance, aggregated, 'centroid')\n",
    "\n",
    "        if(content_based_result[i][4]<=0):\n",
    "            result.append([content_based_result[i][2],content_based_result[i][3],content_based_result[i][4]*importance_defuzz])\n",
    "        else:\n",
    "            result.append([content_based_result[i][2],content_based_result[i][3],content_based_result[i][4]*(1+importance_defuzz)])\n",
    "\n",
    "        # Visualize this\n",
    "        if\n",
    "        importance_activation = fuzz.interp_membership(importance, aggregated, importance_defuzz)\n",
    "        fig, ax0 = plt.subplots(figsize=(8, 3))\n",
    "\n",
    "        ax0.plot(importance, importance_very_low, 'b', linewidth=0.5, linestyle='--', )\n",
    "        ax0.plot(importance, importance_low, 'g', linewidth=0.5, linestyle='--')\n",
    "        ax0.plot(importance, importance_medium, 'r', linewidth=0.5, linestyle='--')\n",
    "        ax0.plot(importance, importance_high, 'b', linewidth=0.5, linestyle='--')\n",
    "        ax0.plot(importance, importance_very_high, 'g', linewidth=0.5, linestyle='--')\n",
    "        ax0.fill_between(importance, importance0, aggregated, facecolor='Orange', alpha=0.7)\n",
    "        ax0.plot([importance_defuzz, importance_defuzz], [0, importance_activation], 'k', linewidth=1.5, alpha=0.9)\n",
    "        ax0.set_title('Aggregated membership and result (line) for '+content_based_result[i][3])\n",
    "    sorted(result, key=lambda item: item[2], reverse=True)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeFormat(user_id, result):\n",
    "    x = list(map(lambda line : str(user_id) + \" & \" + \" & \".join(list(map(lambda elem : str(elem), line))) + \" \\\\\\\\\\n\", result))\n",
    "    return \"\".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "616 & 296 & Pulp Fiction (1994) & 0.05334486658081126 \\\\\n",
      "616 & 1732 & Big Lebowski, The (1998) & 0.03958759479968693 \\\\\n",
      "616 & 593 & Silence of the Lambs, The (1991) & 0.03809464112507027 \\\\\n",
      "616 & 50 & Usual Suspects, The (1995) & 0.037907708091947746 \\\\\n",
      "616 & 48385 & Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan (2006) & 0.03731959023145763 \\\\\n",
      "616 & 924 & 2001: A Space Odyssey (1968) & 0.03668568918320433 \\\\\n",
      "616 & 5618 & Spirited Away (Sen to Chihiro no kamikakushi) (2001) & 0.035857532562342066 \\\\\n",
      "616 & 1073 & Willy Wonka & the Chocolate Factory (1971) & 0.03308796981680937 \\\\\n",
      "616 & 4878 & Donnie Darko (2001) & 0.031779471016178604 \\\\\n",
      "616 & 1206 & Clockwork Orange, A (1971) & 0.031684136788619796 \\\\\n",
      "616 & 8874 & Shaun of the Dead (2004) & 0.031082761125500614 \\\\\n",
      "616 & 2710 & Blair Witch Project, The (1999) & 0.031028201096381273 \\\\\n",
      "616 & 32 & Twelve Monkeys (a.k.a. 12 Monkeys) (1995) & 0.030670800385262098 \\\\\n",
      "616 & 1270 & Back to the Future (1985) & 0.03057676935186751 \\\\\n",
      "616 & 46578 & Little Miss Sunshine (2006) & 0.030382818068401513 \\\\\n",
      "616 & 4226 & Memento (2000) & 0.030084563098584045 \\\\\n",
      "616 & 608 & Fargo (1996) & 0.02954601426139513 \\\\\n",
      "616 & 2012 & Back to the Future Part III (1990) & 0.029454401784294898 \\\\\n",
      "616 & 4979 & Royal Tenenbaums, The (2001) & 0.028288729302405953 \\\\\n",
      "616 & 5952 & Lord of the Rings: The Two Towers, The (2002) & 0.0280636237203554 \\\\\n",
      "\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "616 & 0245429 & 0.9999999999999999 & 5618 & Spirited Away (Sen to Chihiro no kamikakushi) (2001) & 0.035857532562342066 \\\\\n",
      "616 & 0167261 & 0.16604216371896502 & 5952 & Lord of the Rings: The Two Towers, The (2002) & 0.0280636237203554 \\\\\n",
      "616 & 0185937 & 0.09495160102051256 & 2710 & Blair Witch Project, The (1999) & 0.031028201096381273 \\\\\n",
      "616 & 0099088 & 0.08962008381990043 & 2012 & Back to the Future Part III (1990) & 0.029454401784294898 \\\\\n",
      "616 & 0365748 & 0.06828053863241555 & 8874 & Shaun of the Dead (2004) & 0.031082761125500614 \\\\\n",
      "616 & 0066921 & 0.05880609201049225 & 1206 & Clockwork Orange, A (1971) & 0.031684136788619796 \\\\\n",
      "616 & 0116282 & 0.057283457164564636 & 608 & Fargo (1996) & 0.02954601426139513 \\\\\n",
      "616 & 0067992 & 0.05262060265856741 & 1073 & Willy Wonka & the Chocolate Factory (1971) & 0.03308796981680937 \\\\\n",
      "616 & 0209144 & 0.05257255453281975 & 4226 & Memento (2000) & 0.030084563098584045 \\\\\n",
      "616 & 0088763 & 0.04967153216861018 & 1270 & Back to the Future (1985) & 0.03057676935186751 \\\\\n",
      "616 & 0265666 & 0.04415204684654249 & 4979 & Royal Tenenbaums, The (2001) & 0.028288729302405953 \\\\\n",
      "616 & 0449059 & 0.040255833456601914 & 46578 & Little Miss Sunshine (2006) & 0.030382818068401513 \\\\\n",
      "616 & 0102926 & 0.03995972932430571 & 593 & Silence of the Lambs, The (1991) & 0.03809464112507027 \\\\\n",
      "616 & 0118715 & 0.03920761883123017 & 1732 & Big Lebowski, The (1998) & 0.03958759479968693 \\\\\n",
      "616 & 0114814 & 0.03555750902119712 & 50 & Usual Suspects, The (1995) & 0.037907708091947746 \\\\\n",
      "616 & 0443453 & 0.03482563355785318 & 48385 & Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan (2006) & 0.03731959023145763 \\\\\n",
      "616 & 0246578 & 0.03391453747655195 & 4878 & Donnie Darko (2001) & 0.031779471016178604 \\\\\n",
      "616 & 0062622 & 0.031937777342564616 & 924 & 2001: A Space Odyssey (1968) & 0.03668568918320433 \\\\\n",
      "616 & 0114746 & 0.03155606894305671 & 32 & Twelve Monkeys (a.k.a. 12 Monkeys) (1995) & 0.030670800385262098 \\\\\n",
      "616 & 0110912 & 0.025700610074296044 & 296 & Pulp Fiction (1994) & 0.05334486658081126 \\\\\n",
      "\n",
      "--------------------------------------\n",
      "616 & 5618 & Spirited Away (Sen to Chihiro no kamikakushi) (2001) & 0.05229093164520471 \\\\\n",
      "616 & 5952 & Lord of the Rings: The Two Towers, The (2002) & 0.04196191604813887 \\\\\n",
      "616 & 2710 & Blair Witch Project, The (1999) & 0.04140487997480679 \\\\\n",
      "616 & 2012 & Back to the Future Part III (1990) & 0.0394145319181553 \\\\\n",
      "616 & 8874 & Shaun of the Dead (2004) & 0.03756868293341622 \\\\\n",
      "616 & 1206 & Clockwork Orange, A (1971) & 0.038769163803969844 \\\\\n",
      "616 & 608 & Fargo (1996) & 0.03628188459744516 \\\\\n",
      "616 & 1073 & Willy Wonka & the Chocolate Factory (1971) & 0.04108737973622992 \\\\\n",
      "616 & 4226 & Memento (2000) & 0.0373628623547624 \\\\\n",
      "616 & 1270 & Back to the Future (1985) & 0.037893527116737116 \\\\\n",
      "616 & 4979 & Royal Tenenbaums, The (2001) & 0.03490271421443353 \\\\\n",
      "616 & 46578 & Little Miss Sunshine (2006) & 0.037371004127636914 \\\\\n",
      "616 & 593 & Silence of the Lambs, The (1991) & 0.046845111675833405 \\\\\n",
      "616 & 1732 & Big Lebowski, The (1998) & 0.04865211400434309 \\\\\n",
      "616 & 50 & Usual Suspects, The (1995) & 0.046454887309376185 \\\\\n",
      "616 & 48385 & Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan (2006) & 0.045707381196635866 \\\\\n",
      "616 & 4878 & Donnie Darko (2001) & 0.038895281401491004 \\\\\n",
      "616 & 924 & 2001: A Space Odyssey (1968) & 0.04483083682593156 \\\\\n",
      "616 & 32 & Twelve Monkeys (a.k.a. 12 Monkeys) (1995) & 0.037469878314368 \\\\\n",
      "616 & 296 & Pulp Fiction (1994) & 0.06490512083316247 \\\\\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_id = 616\n",
    "\n",
    "print(writeFormat(user_id, collaborative_filtering(user_id)))\n",
    "print(\"--------------------------------------\")\n",
    "#print(writeFormat(user_id, content_based(user_id, False)))\n",
    "print(\"--------------------------------------\")\n",
    "print(writeFormat(user_id, content_based(user_id, True)))\n",
    "print(\"--------------------------------------\")\n",
    "print(writeFormat(user_id, fuzzy(user_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "recommender.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

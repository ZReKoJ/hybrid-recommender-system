{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLNqYBgmGIyf"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "9hzzVAc5GBA_"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "import json\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "import threading\n",
    "import copy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize.regexp import (WordPunctTokenizer,wordpunct_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vH15YyQGM1I"
   },
   "source": [
    "Links for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Kj91r1mXGRPD"
   },
   "outputs": [],
   "source": [
    "url_links = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/links.csv'\n",
    "url_movies = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/movies.csv'\n",
    "url_ratings = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/ratings.csv'\n",
    "url_tags = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/tags.csv'\n",
    "url_genres = 'https://zrekoj.github.io/hybrid-recommender-system/dataset/user_genre.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7uqFvueudeU"
   },
   "source": [
    "Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JUqBO0kkucLC"
   },
   "outputs": [],
   "source": [
    "ds_links = pd.read_csv(url_links, dtype = str)\n",
    "ds_movies = pd.read_csv(url_movies, dtype = str)\n",
    "ds_ratings = pd.read_csv(url_ratings)\n",
    "ds_tags = pd.read_csv(url_tags, dtype = str)\n",
    "ds_user_genres = pd.read_json(url_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uxrT24702OZK"
   },
   "outputs": [],
   "source": [
    "list_movies_id = list(ds_movies['movieId'].unique())\n",
    "list_title_id = ds_movies['title'].tolist()\n",
    "list_users_id = list(ds_ratings['userId'].unique())\n",
    "list_movies_imdbid = ds_links['imdbId'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCHdeaAxoy5i"
   },
   "source": [
    "Input ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "id": "O_MY4EUno1II",
    "outputId": "2507215a-d421-4bff-dae7-7bdce5d22934"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate the movie 13th (2016) from 0.5 to 5.0 or say 0.0 if you haven't seen it :9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\zihao\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    884\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m                 \u001b[0mident\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zihao\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\jupyter_client\\session.py\u001b[0m in \u001b[0;36mrecv\u001b[1;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[0;32m    802\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 803\u001b[1;33m             \u001b[0mmsg_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    804\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zihao\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[1;34m(self, flags, copy, track)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \"\"\"\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mparts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;31m# have first part already, only loop while more to receive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zihao\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-255a398f92c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[1;32mwhile\u001b[0m \u001b[0mlist_movies_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmovie_ids\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_movies_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m   \u001b[0mrating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Rate the movie '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_title_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' from 0.5 to 5.0 or say 0.0 if you haven\\'t seen it :'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m   \u001b[0mmovie_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_movies_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m   \u001b[0mratings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zihao\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[0mpassword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m         )\n\u001b[0;32m    862\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zihao\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    888\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    891\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "movie_ids=[]\n",
    "ratings=[]\n",
    "\n",
    "for i in range(10):\n",
    "  index=random.randint(0,len(list_movies_id)-1)\n",
    "  while list_movies_id[index] in movie_ids:\n",
    "    index=random.randint(0,len(list_movies_id)-1)\n",
    "  rating = float(input('Rate the movie '+str(list_title_id[index])+' from 0.5 to 5.0 or say 0.0 if you haven\\'t seen it :'))\n",
    "  movie_ids.append(list_movies_id[index])\n",
    "  ratings.append(rating)\n",
    "print(movie_ids)\n",
    "print(ratings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfYUIC48Cta9"
   },
   "source": [
    "Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zY9Fa_VcLVa"
   },
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "szoO5K7IKg5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Film-Noir', 'Animation', 'Musical', 'Children', 'Drama', 'War'], ['Comedy', 'Sci-Fi', 'Mystery', 'Thriller', 'Horror'])\n"
     ]
    }
   ],
   "source": [
    "def getGenresByUser(user_id, ds=ds_user_genres):\n",
    "  return ds[user_id]['like'], ds[user_id]['dislike']\n",
    "\n",
    "print(getGenresByUser(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KRrF5bPD_bZs"
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "\n",
    "test_ratio = 0.2 #fraction of data to be used as test set.\n",
    "no_of_features = [8,10,12,14,17,20] # to test the performance over a different number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HVL4pD2A9zIu"
   },
   "outputs": [],
   "source": [
    "ds_ratings['userId'] = ds_ratings['userId'].astype('str')\n",
    "ds_ratings['movieId'] = ds_ratings['movieId'].astype('str')\n",
    "\n",
    "users = ds_ratings['userId'].unique() #list of all users\n",
    "movies = ds_ratings['movieId'].unique() #list of all movies\n",
    "\n",
    "test = pd.DataFrame(columns=ds_ratings.columns)\n",
    "train = pd.DataFrame(columns=ds_ratings.columns)\n",
    "\n",
    "for u in users:\n",
    "  temp = ds_ratings[ds_ratings['userId'] == u]\n",
    "  n = len(temp)\n",
    "  test_size = int(test_ratio*n)\n",
    "\n",
    "  temp = temp.sort_values('timestamp').reset_index()\n",
    "  temp.drop('index', axis=1, inplace=True)\n",
    "\n",
    "  dummy_test = temp.loc[n-1-test_size :]\n",
    "  dummy_train = temp.loc[: n-2-test_size]\n",
    "    \n",
    "  test = pd.concat([test, dummy_test])\n",
    "  train = pd.concat([train, dummy_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GvRLP6__-98r"
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm\n",
    "\n",
    "def create_user_item_matrix(data, formatizer = {'user':0, 'item': 1, 'value': 2}):\n",
    "    itemField = formatizer['item']\n",
    "    userField = formatizer['user']\n",
    "    valueField = formatizer['value']\n",
    "\n",
    "    \n",
    "    userList = data.iloc[:,userField].tolist()\n",
    "    itemList = data.iloc[:,itemField].tolist()\n",
    "    valueList = data.iloc[:,valueField].tolist()\n",
    "\n",
    "    users = list(set(data.iloc[:,userField]))\n",
    "    items = list(set(data.iloc[:,itemField]))\n",
    "\n",
    "    users_index = {users[i]: i for i in range(len(users))}\n",
    "\n",
    "    pd_dict = {item: [np.nan for i in range(len(users))] for item in items}\n",
    "\n",
    "    for i in range(0,len(data)):\n",
    "      item = itemList[i]\n",
    "      user = userList[i]\n",
    "      value = valueList[i]\n",
    "\n",
    "      pd_dict[item][users_index[user]] = value\n",
    "    \n",
    "    X = pd.DataFrame(pd_dict)\n",
    "    X.index = users\n",
    "\n",
    "    itemcols = list(X.columns)\n",
    "    items_index = {itemcols[i]: i for i in range(len(itemcols))}\n",
    "\n",
    "    # users_index gives us a mapping of user_id to index of user\n",
    "    # items_index provides the same for items\n",
    "\n",
    "    return X, users_index, items_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tr-QPs9A_K6a"
   },
   "outputs": [],
   "source": [
    "def svd(train, k):\n",
    "    utilMat = np.array(train)\n",
    "\n",
    "    # the nan or unavailable entries are masked\n",
    "    mask = np.isnan(utilMat)\n",
    "    masked_arr = np.ma.masked_array(utilMat, mask)\n",
    "\n",
    "    item_means = np.mean(masked_arr, axis=0)\n",
    "\n",
    "    # nan entries will replaced by the average rating for each item\n",
    "    utilMat = masked_arr.filled(item_means)\n",
    "    x = np.tile(item_means, (utilMat.shape[0],1))\n",
    "\n",
    "    # we remove the per item average from all entries.\n",
    "    # the above mentioned nan entries will be essentially zero now\n",
    "    utilMat = utilMat - x\n",
    "\n",
    "    # The magic happens here. U and V are user and item features\n",
    "    U, s, V=np.linalg.svd(utilMat, full_matrices=False)\n",
    "    s=np.diag(s)\n",
    "\n",
    "    # we take only the k most significant features\n",
    "    s=s[0:k,0:k]\n",
    "    U=U[:,0:k]\n",
    "    V=V[0:k,:]\n",
    "\n",
    "    s_root=sqrtm(s)\n",
    "    Usk=np.dot(U,s_root)\n",
    "    skV=np.dot(s_root,V)\n",
    "    UsV = np.dot(Usk, skV)\n",
    "    UsV = UsV + x\n",
    "    return UsV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3sG_ANug_Sj_"
   },
   "outputs": [],
   "source": [
    "def rmse(true, pred):\n",
    "    x = true - pred\n",
    "    return sum([xi*xi for xi in x])/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zalPgxeI_Wh-"
   },
   "outputs": [],
   "source": [
    "uiMat, users_index, items_index = create_user_item_matrix(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ixHyBKd3_q5f",
    "outputId": "f123df74-c648-4f27-ff56-5e9628d24459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0037878092372208\n",
      "1.0036410576245884\n",
      "1.0047280968304317\n",
      "1.0053854409715488\n",
      "1.0046710025637098\n",
      "1.0056567229647215\n"
     ]
    }
   ],
   "source": [
    "#svd evaluation\n",
    "for f in no_of_features:\n",
    "  svdout = svd(uiMat, k=f)\n",
    "  pred = [] #to store the predicted ratings\n",
    "  for _,row in test.iterrows():\n",
    "    user = row['userId']\n",
    "    item = row['movieId']\n",
    "    u_index = users_index[user]\n",
    "    \n",
    "    if item in items_index:\n",
    "      i_index = items_index[item]\n",
    "      pred_rating = svdout[u_index, i_index]\n",
    "    else:\n",
    "      pred_rating = np.mean(svdout[u_index, :])\n",
    "    pred.append(pred_rating)\n",
    "  print(rmse(test['rating'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yYdr_ohfTZwz"
   },
   "outputs": [],
   "source": [
    "def filter(u_id, user_genres = ds_user_genres, item_genres = ds_movies[\"genres\"]):\n",
    "  u_likes = user_genres[u_id][\"like\"]\n",
    "  u_detest = user_genres[u_id][\"dislike\"]\n",
    "\n",
    "  useritemMat, users_index, items_index = create_user_item_matrix(ds_ratings)\n",
    "  svdout = svd(useritemMat, k=12)\n",
    "  user_pos = users_index[str(u_id)]\n",
    "  result = []\n",
    "  for i in range(len(svdout[user_pos])):\n",
    "\n",
    "    if str(i) in items_index:\n",
    "      i_id = items_index[str(i)]\n",
    "      i_genre = item_genres[i_id].split(sep=\"|\")\n",
    "\n",
    "      checkDislike =  any(item in i_genre for item in u_detest)\n",
    "      if checkDislike is False:\n",
    "        new_rate = svdout[user_pos][i]\n",
    "\n",
    "        checkLike =  any(item in i_genre for item in u_likes)\n",
    "        if checkLike is True:\n",
    "          new_rate = new_rate + 2\n",
    "        result.append([ds_movies['movieId'][i_id], ds_movies[\"title\"][i_id], new_rate])\n",
    " \n",
    "  return sorted(result, key=lambda item: item[2], reverse=True)[:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "e7JekM73eErX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass\n"
     ]
    }
   ],
   "source": [
    "check =  any(item in [\"Horror\", \"Comedy\"] for item in [\"Comedy\", \"Sci-Fi\", \"Mystery\", \"Thriller\"])\n",
    "if check is True:\n",
    "    print(\"pass\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ilIi5vZJbgd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['25937', 'Easter Parade (1948)', 7.0],\n",
       " ['385', 'Man of No Importance, A (1994)', 7.0],\n",
       " ['6832', 'Regarding Henry (1991)', 7.0],\n",
       " ['6216', 'Nowhere in Africa (Nirgendwo in Afrika) (2001)', 7.0],\n",
       " ['64499', 'Che: Part One (2008)', 7.0],\n",
       " ['1507', 'Paradise Road (1997)', 7.0],\n",
       " ['1415', 'Thieves (Voleurs, Les) (1996)', 7.0],\n",
       " ['1401', 'Ghosts of Mississippi (1996)', 7.0],\n",
       " ['3000', 'Princess Mononoke (Mononoke-hime) (1997)', 7.0],\n",
       " ['5471', 'Perfect (1985)', 7.0],\n",
       " ['4896',\n",
       "  \"Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001)\",\n",
       "  7.0],\n",
       " ['55274', 'Elizabeth: The Golden Age (2007)', 7.0],\n",
       " ['7243', \"Intolerance: Love's Struggle Throughout the Ages (1916)\", 7.0],\n",
       " ['851', 'Basquiat (1996)', 7.0],\n",
       " ['25753', 'Greed (1924)', 7.0],\n",
       " ['918', 'Meet Me in St. Louis (1944)', 7.0],\n",
       " ['114795', 'Dracula Untold (2014)', 7.0],\n",
       " ['54997', '3:10 to Yuma (2007)', 7.0],\n",
       " ['157122', 'The Man Who Knew Infinity (2016)', 7.0],\n",
       " ['6777', 'Judgment at Nuremberg (1961)', 7.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7wSplTSCq7_"
   },
   "source": [
    "**Content-based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "INdMx6n9Cs_P"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, np.transpose(v)) / np.linalg.norm(u) * np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1Yf5cIo8rOe1"
   },
   "outputs": [],
   "source": [
    "def readPlots(movies):\n",
    "  plots = []\n",
    "  for movie in movies:\n",
    "    url = 'https://zrekoj.github.io/hybrid-recommender-system/movies/'+ str(movie)+'.json'\n",
    "    archivo = pd.read_json(url)\n",
    "    plots.append([archivo['Plot'].tolist()[0]])\n",
    "  return plots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqRoOqAubo6O"
   },
   "source": [
    "Build Frecuency Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LNUIYzosbxbh"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(doc):\n",
    " stopset = set(stopwords.words('english'))\n",
    " stemmer = SnowballStemmer('english',ignore_stopwords=True)\n",
    " tokens = wordpunct_tokenize(doc)\n",
    " clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    " stemmed_text = [stemmer.stem(word) for word in clean]\n",
    " return stemmed_text\n",
    " \n",
    " \n",
    "def create_freq_dict(movieIds, preprocessed_texts):\n",
    " i=0\n",
    " print(\"Creating frequency dictionary\")\n",
    " freqDict_list=[]\n",
    " all_tokens=set()\n",
    " #for j in range(0, len(preprocessed_texts)-1):\n",
    "   #for token in preprocessed_texts[j]:\n",
    "     #all_tokens.add(token)\n",
    " for tokens in preprocessed_texts:\n",
    "  freq_dict={}\n",
    "  \n",
    "  for token in tokens:\n",
    "    if token in freq_dict:\n",
    "     freq_dict[token]+=1\n",
    "    else:\n",
    "     freq_dict[token]=1\n",
    "  #for token in all_tokens:\n",
    "    #if token not in freq_dict:\n",
    "       #freq_dict[token]=0\n",
    "\n",
    "  temp={'term_id':i,'movie_id':movieIds[i],'freq_dict':freq_dict,'doc_length':len(tokens)}\n",
    "  i+=1\n",
    "  freqDict_list.append(temp)\n",
    " return freqDict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ4w4ThMbyiK"
   },
   "source": [
    "Calculate TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxnJAY9xiQA2"
   },
   "source": [
    "Content-Based application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jdp1gOGyb3kB"
   },
   "outputs": [],
   "source": [
    "def computeTF(freqDict_list):\n",
    " TF_scores=[]\n",
    " print(\"Calculating TF\")\n",
    " for tempDict in freqDict_list:\n",
    "  id=tempDict['term_id']\n",
    "  name=tempDict['movie_id']\n",
    "  for k in tempDict['freq_dict']:\n",
    "   temp={'term_id':id, 'movie_id':name,'TF_score':tempDict['freq_dict'][k]/tempDict['doc_length'],'key':k}\n",
    "   TF_scores.append(temp)\n",
    " return TF_scores\n",
    " \n",
    "def computeIDF(freqDict_list):\n",
    "  IDF_scores=[]\n",
    "  print(\"Calculating IDF\")\n",
    "  counter=-1\n",
    "  for dict in freqDict_list:\n",
    "   counter+=1\n",
    "   for k in dict['freq_dict'].keys():\n",
    "    count=sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "    temp= {'term_id': counter, 'IDF_score':math.log(dict['doc_length'])/count,'key':k}\n",
    "    IDF_scores.append(temp)\n",
    "  return IDF_scores\n",
    " \n",
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    " TF_IDF_scores=[]\n",
    " print(\"Calculating TF-IDF\")\n",
    " \n",
    " for j in IDF_scores:\n",
    "  for i in TF_scores:\n",
    "   if j['key']==i['key'] and j['term_id']==i['term_id']:\n",
    "    temp={'term_id':j['term_id'], 'movie_id':i['movie_id'], 'TFIDF_score': j['IDF_score']*i['TF_score'], 'key':i['key']}\n",
    "    TF_IDF_scores.append(temp)\n",
    " return TF_IDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFknV71aVRK6",
    "outputId": "b03ae66a-e902-4c96-a2f9-69d1ec9d4ac0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Zihao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0113497 3.543544460041931e-06\n",
      "0114885 7.309774086299786e-06\n",
      "0113041 6.757274151956908e-06\n",
      "0114319 3.2347987238123826e-06\n",
      "0112302 1.8611607309117507e-05\n",
      "0114576 6.34765936756794e-06\n"
     ]
    }
   ],
   "source": [
    "#zihao\n",
    "\n",
    "import requests \n",
    "\n",
    "user_id = 1\n",
    "hybrid = False\n",
    "\n",
    "movie_similarities=[]\n",
    "\n",
    "#Get imbdIds of user movies\n",
    "user_movies = list(\n",
    "    map(\n",
    "        lambda movie_id : str(ds_links.loc[ds_links['movieId'] == str(movie_id), 'imdbId'].values[0]), \n",
    "        ds_ratings.loc[ds_ratings['userId'] == str(user_id), 'movieId'].tolist()\n",
    "    )\n",
    ")\n",
    "\n",
    "#collaborative_filtering_result = filter(user_id)\n",
    "movies=[]\n",
    "\n",
    "#If we are using content-based as part of the hybrid filter, it uses the results of the collaborative filtering\n",
    "#Otherwise it uses all movies\n",
    "if(hybrid):\n",
    "    movies = list(\n",
    "        map(\n",
    "            lambda result : ds_links.loc[ds_links['movieId'] == str(result[0]), 'imdbId'].values[0], \n",
    "            collaborative_filtering_result\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    movies=[str(movie_id) for movie_id in list_movies_imdbid if movie_id not in user_movies]\n",
    "    \n",
    "response = requests.get('https://zrekoj.github.io/hybrid-recommender-system/user_frequency/'+ str(user_id)+'.json') \n",
    "user_frequence_dict = response.json()\n",
    "\n",
    "def computeTFIDF(user_frequence_dict):\n",
    "    copied = copy.deepcopy(user_frequence_dict)\n",
    "    for frequency in copied.values():\n",
    "        length = len(frequency)\n",
    "        total_count = sum(frequency.values())\n",
    "        for key in frequency:\n",
    "            frequency[key] = (frequency[key] / length) * (math.log(length) / total_count)\n",
    "    return copied\n",
    "            \n",
    "for movie in movies:\n",
    "    MaxSim = 0\n",
    "    \n",
    "    movie_frequence_dict = requests.get('https://zrekoj.github.io/hybrid-recommender-system/frequency/'+ str(movie)+'.json').json() \n",
    "    \n",
    "    user_frequence_dict[str(movie)] = movie_frequence_dict\n",
    "    TFIDF = computeTFIDF(user_frequence_dict)\n",
    "    user_frequence_dict.pop(str(movie), None)\n",
    "    \n",
    "    TFIDF_highest = TFIDF[str(movie)]\n",
    "    \n",
    "    for user_movie in user_movies[:10]:\n",
    "        TFIDF_user =  TFIDF[str(user_movie)]\n",
    "        TFIDF_user_values = list(\n",
    "            map(\n",
    "                lambda key : TFIDF_user[key] if key in TFIDF_user.keys() else 0,\n",
    "                TFIDF_highest.keys()\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        similarity = cosine_similarity(list(TFIDF_highest.values()), TFIDF_user_values)        \n",
    "        \n",
    "        if(similarity > MaxSim):\n",
    "            MaxSim = similarity\n",
    "    movie_similarities.append([movie, MaxSim])\n",
    "    print(movie, MaxSim)\n",
    "\n",
    "sorted(movie_similarities, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(movie_similarities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvvBWUOud1mz",
    "outputId": "d63f055f-3fd0-4e4a-979c-be80feee7849"
   },
   "outputs": [],
   "source": [
    "def content_based(user_id, hybrid=True):\n",
    "\n",
    "  movie_similarities=[]\n",
    "  temp_user_movies=[]\n",
    "  temp_movies=[]\n",
    "  user_movies=[]\n",
    "  #Get movies of selected user\n",
    "  for k in range(0,len(ds_ratings)-1):\n",
    "    if str(ds_ratings['userId'][k])==str(user_id):\n",
    "      temp_user_movies.append(ds_ratings['movieId'][k])\n",
    "  #Get imbdIds of user movies\n",
    "  for k in range(0,len(ds_links)-1):\n",
    "    if str(ds_links['movieId'][k]) in temp_user_movies:\n",
    "      user_movies.append(str(ds_links['imdbId'][k]))\n",
    "  \n",
    "  movies=[]\n",
    "  \n",
    "  #If we are using content-based as part of the hybrid filter, it uses the results of the collaborative filtering\n",
    "  #Otherwise it uses all movies\n",
    "  if(hybrid):\n",
    "    temp_movies=[str(x[0]) for x in filter(user_id)]\n",
    "    for k in range(0,len(ds_links)):\n",
    "      if str(ds_links['movieId'][k]) in temp_movies:\n",
    "        movies.append(str(ds_links['imdbId'][k]))\n",
    "  else:\n",
    "    movies=[str(movie_id) for movie_id in list_movies_imdbid if movie_id not in user_movies]\n",
    "  #Preprocess user movies\n",
    "  preprocessed_texts=[preprocess_text(text[0]) for text in readPlots(user_movies)]\n",
    "\n",
    "  for i in range(0,len(movies)-1):\n",
    "    MaxSim=0\n",
    "    user_movies.append(movies[i])\n",
    "    preprocessed_texts.append(preprocess_text(readPlots([movies[i]])[0][0]))\n",
    "    freqDict=create_freq_dict(user_movies, preprocessed_texts)\n",
    "    user_movies.pop(-1)\n",
    "    preprocessed_texts.pop(-1)\n",
    "    TFIDF=computeTFIDF(computeTF(freqDict),computeIDF(freqDict))\n",
    "\n",
    "    TFIDF_highest_temp=[[y[\"key\"],y[\"TFIDF_score\"]] for y in TFIDF if y[\"movie_id\"]==movies[i]]\n",
    "    TFIDF_highest=[y[1] for y in TFIDF_highest_temp]\n",
    "    #TFIDF_highest=list(map(lambda y : y[\"TFIDF_score\"] if y[\"movie_id\"]==movies[i] else 0, TFIDF))\n",
    "    \n",
    "    for j in range(0,9):\n",
    "      \n",
    "      TFIDF_user_temp={x[\"key\"]:x[\"TFIDF_score\"] for x in TFIDF if x[\"movie_id\"]==user_movies[j]}\n",
    "      TFIDF_user=[]\n",
    "      for x in TFIDF_highest_temp:\n",
    "        if x[0] in TFIDF_user_temp.keys():\n",
    "          TFIDF_user.append(TFIDF_user_temp[x[0]])\n",
    "        else:\n",
    "          TFIDF_user.append(0)\n",
    "\n",
    "      #TFIDF_user=list(map(lambda y : y[\"TFIDF_score\"] if y[\"movie_id\"]==user_movies[i] else 0, TFIDF))\n",
    "      \n",
    "      similarity=cosine_similarity(TFIDF_highest,TFIDF_user)\n",
    "      if(similarity>MaxSim):\n",
    "        MaxSim=similarity\n",
    "    movie_similarities.append([movies[i],MaxSim])\n",
    "  return sorted(movie_similarities, key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(content_based(1))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h66pQrMvx3oC"
   },
   "source": [
    "Fuzzy Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "AooHYk2M_pY4",
    "outputId": "53624136-87cb-487c-9607-a361ea979263"
   },
   "outputs": [],
   "source": [
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4V7Sd0pyT-O"
   },
   "outputs": [],
   "source": [
    "inp1 = ctrl.Antecedent(np.arange(0, 5, 1), 'average_rating')\n",
    "inp2 = ctrl.Antecedent(np.arange(0, 350, 1), 'total_ratings')\n",
    "inp3 = ctrl.Antecedent(np.arange(0, 1, 1), 'similarity')\n",
    "out = ctrl.Consequent(np.arange(0, 1, 1), 'importance')\n",
    "\n",
    "out['low'] = fuzz.trimf(out.universe, [0, 0, .2])\n",
    "out['medium'] = fuzz.trimf(out.universe, [0, .6, 25])\n",
    "out['high'] = fuzz.trimf(out.universe, [.6, 1, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "recommender.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
